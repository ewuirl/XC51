{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serves as a baseline for more complicated models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing required packages and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../')\n",
    "from simCRN.multivariate_reg import read_eq_data_file, convert_np2df, get_stats, Z_normalize_data, min_max_normalize, prep_data, plot_data, plot_predict, plot_true_and_pred, plot_error_hist, plot_true_v_error, plot_residuals, subset\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_docs as tfdocs\n",
    "import tensorflow_docs.plots\n",
    "import tensorflow_docs.modeling\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ci_all_array, Am_array, Cmin, Cmax, Ai = read_eq_data_file('../4-4-2-asym-AB-AC.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Splitting into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(Am_array, Ci_all_array, test_size=0.2, random_state=0)\n",
    "\n",
    "# Z normalizing\n",
    "X_scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Ridge(alpha=1.0) # The linear regression model\n",
    "\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_hat_train = clf.predict(X_train_scaled)\n",
    "y_hat_test = clf.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the performance of the linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error as mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MAE on the training data for C₁ is 7.71e-08\n",
      "The MAE on the training data for C₂ is 2.6e-08\n",
      "The MAE on the test data for C₁ is 7.5e-08\n",
      "The MAE on the test data for C₂ is 2.74e-08\n",
      "\n",
      "The average value of C₁ is 7.61e-07\n",
      "The average value of C₂ is 7.5e-07\n",
      "\n",
      "For the test data, MAE/mean for C₁ is 0.0986\n",
      "For the test data, MAE/mean for C₂ is 0.0365\n"
     ]
    }
   ],
   "source": [
    "train_mae = mae(y_train, y_hat_train, multioutput='raw_values')\n",
    "test_mae = mae(y_test, y_hat_test, multioutput='raw_values')\n",
    "\n",
    "print(f'The MAE on the training data for C₁ is {train_mae[0]:.3}') # 3 significant figures\n",
    "print(f'The MAE on the training data for C₂ is {train_mae[1]:.3}')\n",
    "print(f'The MAE on the test data for C₁ is {test_mae[0]:.3}')\n",
    "print(f'The MAE on the test data for C₂ is {test_mae[1]:.3}')\n",
    "\n",
    "print() # new line\n",
    "\n",
    "# Contextualizing with the mean of C₁ and C₂\n",
    "means = np.mean(Ci_all_array, axis=0)\n",
    "print(f'The average value of C₁ is {means[0]:.3}')\n",
    "print(f'The average value of C₂ is {means[1]:.3}')\n",
    "\n",
    "print() # new line\n",
    "\n",
    "print(f'For the test data, MAE/mean for C₁ is {test_mae[0]/means[0]:.3}')\n",
    "print(f'For the test data, MAE/mean for C₂ is {test_mae[1]/means[1]:.3}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error as mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MSE on the training data for C₁ is 9.09e-15\n",
      "The MSE on the training data for C₂ is 1.06e-15\n",
      "The MSE on the test data for C₁ is 8.67e-15\n",
      "The MSE on the test data for C₂ is 1.16e-15\n"
     ]
    }
   ],
   "source": [
    "train_mse = mse(y_train, y_hat_train, multioutput='raw_values')\n",
    "test_mse = mse(y_test, y_hat_test, multioutput='raw_values')\n",
    "\n",
    "print(f'The MSE on the training data for C₁ is {train_mse[0]:.3}') # 3 significant figures\n",
    "print(f'The MSE on the training data for C₂ is {train_mse[1]:.3}')\n",
    "print(f'The MSE on the test data for C₁ is {test_mse[0]:.3}')\n",
    "print(f'The MSE on the test data for C₂ is {test_mse[1]:.3}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The R² on the training data for C₁ is 0.951\n",
      "The R² on the training data for C₂ is 0.994\n",
      "The R² on the test data for C₁ is 0.955\n",
      "The R² on the test data for C₂ is 0.994\n"
     ]
    }
   ],
   "source": [
    "train_r2 = r2_score(y_train, y_hat_train, multioutput='raw_values')\n",
    "test_r2 = r2_score(y_test, y_hat_test, multioutput='raw_values')\n",
    "\n",
    "print(f'The R² on the training data for C₁ is {train_r2[0]:.3}') # 3 significant figures\n",
    "print(f'The R² on the training data for C₂ is {train_r2[1]:.3}')\n",
    "print(f'The R² on the test data for C₁ is {test_r2[0]:.3}')\n",
    "print(f'The R² on the test data for C₂ is {test_r2[1]:.3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking to see if hyperoptimization helps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start trials\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 300/300 [00:02<00:00, 111.16trial/s, best loss: 3.008633476602636e-15]\n",
      "Best parameter set: {'reg': 0.002910090441069553, 'reg_type': 1}\n",
      "Best loss from CV: 3e-15\n"
     ]
    }
   ],
   "source": [
    "from hyperopt import hp\n",
    "from hyperopt import fmin, tpe, space_eval, Trials\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "parameter_space =  {\"reg_type\": hp.choice(\"reg_type\", [\"Ridge\", \"Lasso\"]), # L2 vs L1 regularization type\n",
    "                    \"reg\": hp.uniform(\"reg\", 0, 10), # try values between zero (no regularization) and 10 (high regularization)\n",
    "                    }\n",
    "\n",
    "# Evaluation function \n",
    "# args should be a dict, with keys for reg_type and reg\n",
    "def model_eval(args):\n",
    "\n",
    "    '''Take suggested arguments and perform model evaluation'''\n",
    "    \n",
    "    if args[\"reg_type\"] == \"Ridge\":\n",
    "        model = Ridge(alpha=args[\"reg\"])\n",
    "    elif args[\"reg_type\"] == \"Lasso\":\n",
    "        model = Lasso(alpha=args[\"reg\"])\n",
    "    else:\n",
    "        raise Exception(\"Something has gone wrong!\")\n",
    "    \n",
    "    scores = cross_val_score(model, X_train_scaled, y=y_train, scoring='neg_mean_squared_error')\n",
    "        # I believe that neg_mean_squared_error uses a uniform_average of the two outputs\n",
    "        # https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html#sklearn.metrics.mean_squared_error\n",
    "\n",
    "    cv_score = np.mean(scores)\n",
    "\n",
    "    # return the negative of the CV score to ensure we maximize the negative MSE by minimizing the loss\n",
    "    return -cv_score\n",
    "\n",
    "\n",
    "print(\"Start trials\") \n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(model_eval, parameter_space, algo=tpe.suggest, max_evals=300, trials=trials)\n",
    "\n",
    "print(\"Best parameter set: {}\".format(best))\n",
    "print(\"Best loss from CV: {:.2}\".format(trials.best_trial['result']['loss']))\n",
    "\n",
    "########### Code #############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
